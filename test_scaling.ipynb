{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dictionary_learning.dictionary import (\n",
    "    AutoEncoder,\n",
    "    GatedAutoEncoder,\n",
    "    AutoEncoderNew,\n",
    "    JumpReluAutoEncoder,\n",
    ")\n",
    "from dictionary_learning.trainers.top_k import AutoEncoderTopK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "d_model = 100\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "scale = 4\n",
    "\n",
    "x = torch.randn(1000, d_model, device=device)\n",
    "\n",
    "x_scaled = x / scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jumprelu_ae = JumpReluAutoEncoder(activation_dim=d_model, dict_size=d_model * 8, device=device)\n",
    "\n",
    "jumprelu_ae.b_enc.data = torch.randn_like(jumprelu_ae.b_enc.data)\n",
    "jumprelu_ae.b_dec.data = torch.randn_like(jumprelu_ae.b_dec.data)\n",
    "jumprelu_ae.threshold.data = abs(torch.randn_like(jumprelu_ae.threshold.data))\n",
    "\n",
    "reconstruction_1 = jumprelu_ae(x_scaled)\n",
    "\n",
    "def scale_jumprelu(ae: JumpReluAutoEncoder, scale: float):\n",
    "    ae.b_dec.data *= scale\n",
    "    ae.b_enc.data *= scale\n",
    "    ae.threshold.data *= scale\n",
    "\n",
    "print(jumprelu_ae.threshold.mean())\n",
    "scale_jumprelu(jumprelu_ae, (scale))\n",
    "print(jumprelu_ae.threshold.mean())\n",
    "\n",
    "reconstruction_2 = jumprelu_ae(x)\n",
    "\n",
    "reconstruction_1 = reconstruction_1 * scale\n",
    "\n",
    "diff = torch.abs(reconstruction_1 - reconstruction_2)\n",
    "print(f\"max diff: {diff.max()}, mean diff: {diff.mean()}\")\n",
    "\n",
    "assert torch.allclose(reconstruction_1, reconstruction_2, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_ae = GatedAutoEncoder(activation_dim=d_model, dict_size=d_model * 8, device=device)\n",
    "\n",
    "gated_ae.r_mag.data = torch.randn_like(gated_ae.r_mag.data)\n",
    "gated_ae.decoder_bias.data = torch.randn_like(gated_ae.decoder_bias.data)\n",
    "gated_ae.mag_bias.data = torch.randn_like(gated_ae.mag_bias.data)\n",
    "gated_ae.gate_bias.data = torch.randn_like(gated_ae.gate_bias.data)\n",
    "\n",
    "reconstruction_1 = gated_ae(x_scaled)\n",
    "\n",
    "def scale_gated(ae: GatedAutoEncoder, scale: float):\n",
    "    ae.decoder_bias.data *= scale\n",
    "    ae.mag_bias.data *= scale\n",
    "    ae.gate_bias.data *= scale\n",
    "\n",
    "print(gated_ae.r_mag.mean(), gated_ae.decoder_bias.mean(), gated_ae.mag_bias.mean(), gated_ae.gate_bias.mean())\n",
    "scale_gated(gated_ae, (scale))\n",
    "scale_gated(gated_ae, (1 / scale))\n",
    "scale_gated(gated_ae, (scale))\n",
    "\n",
    "\n",
    "print(gated_ae.r_mag.mean(), gated_ae.decoder_bias.mean(), gated_ae.mag_bias.mean(), gated_ae.gate_bias.mean())\n",
    "\n",
    "reconstruction_2 = gated_ae(x)\n",
    "\n",
    "reconstruction_1 = reconstruction_1 * scale\n",
    "\n",
    "diff = torch.abs(reconstruction_1 - reconstruction_2)\n",
    "\n",
    "print(f\"max diff: {diff.max()}, mean diff: {diff.mean()}\")\n",
    "assert torch.allclose(reconstruction_1, reconstruction_2, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_ae = AutoEncoder(activation_dim=d_model, dict_size=d_model * 8)\n",
    "relu_ae = relu_ae.to(device)\n",
    "\n",
    "# relu_ae.encoder.bias.data = torch.randn_like(relu_ae.decoder.bias.data)\n",
    "relu_ae.bias.data = torch.randn_like(relu_ae.bias.data)\n",
    "\n",
    "reconstruction_1 = relu_ae(x_scaled)\n",
    "\n",
    "def scale_relu(ae: AutoEncoder, scale: float):\n",
    "    ae.encoder.bias.data *= scale\n",
    "    ae.bias.data *= scale\n",
    "\n",
    "\n",
    "print(relu_ae.bias.mean())\n",
    "scale_relu(relu_ae, (scale))\n",
    "print(relu_ae.bias.mean())\n",
    "\n",
    "reconstruction_2 = relu_ae(x)\n",
    "\n",
    "reconstruction_1 = reconstruction_1 * scale\n",
    "\n",
    "diff = torch.abs(reconstruction_1 - reconstruction_2)\n",
    "\n",
    "print(f\"max diff: {diff.max()}, mean diff: {diff.mean()}\")\n",
    "assert torch.allclose(reconstruction_1, reconstruction_2, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_ae = AutoEncoderTopK(activation_dim=d_model, dict_size=d_model * 8, k=20)\n",
    "\n",
    "topk_ae.encoder.bias.data = torch.randn_like(topk_ae.encoder.bias.data)\n",
    "topk_ae.b_dec.data = torch.randn_like(topk_ae.b_dec.data)\n",
    "print(topk_ae.threshold)\n",
    "topk_ae.threshold = abs(torch.randn_like(topk_ae.threshold))\n",
    "print(topk_ae.threshold)\n",
    "\n",
    "reconstruction_1 = topk_ae(x_scaled)\n",
    "\n",
    "def scale_topk(ae: AutoEncoderTopK, scale: float):\n",
    "    ae.encoder.bias.data *= scale\n",
    "    ae.b_dec.data *= scale\n",
    "    if ae.threshold >= 0:\n",
    "        ae.threshold *= scale\n",
    "\n",
    "print(topk_ae.encoder.bias.mean(), topk_ae.b_dec.mean())\n",
    "scale_topk(topk_ae, (scale))\n",
    "print(topk_ae.encoder.bias.mean(), topk_ae.b_dec.mean())\n",
    "\n",
    "reconstruction_2 = topk_ae(x)\n",
    "\n",
    "reconstruction_1 = reconstruction_1 * scale\n",
    "\n",
    "diff = torch.abs(reconstruction_1 - reconstruction_2)\n",
    "\n",
    "print(f\"max diff: {diff.max()}, mean diff: {diff.mean()}\")\n",
    "assert torch.allclose(reconstruction_1, reconstruction_2, atol=1e-5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
