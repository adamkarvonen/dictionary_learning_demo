{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from nnsight import LanguageModel\n",
    "import argparse\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import torch.multiprocessing as mp\n",
    "import time\n",
    "import huggingface_hub\n",
    "from datasets import config\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model_name = \"EleutherAI/pythia-1b\"\n",
    "device = \"cuda\"\n",
    "hf_model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    revision=\"step0\",\n",
    "    cache_dir=\"./pythia-1b/step0\",\n",
    ").to(device)\n",
    "\n",
    "model = LanguageModel(model_name, dispatch=True, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)\n",
    "# print(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def copy_weights(source_model, target_model):\n",
    "    \"\"\"\n",
    "    Copy weights from source model to target model, handling the slight structural differences\n",
    "    between HuggingFace and LanguageModel implementations.\n",
    "    \"\"\"\n",
    "    # Create state dict mapping\n",
    "    source_state = source_model.state_dict()\n",
    "    target_state = target_model.state_dict()\n",
    "    \n",
    "    # Copy matching parameters\n",
    "    for target_key in target_state:\n",
    "        if target_key in source_state:\n",
    "            target_state[target_key].copy_(source_state[target_key])\n",
    "    \n",
    "    # Load the updated state dict\n",
    "    target_model.load_state_dict(target_state)\n",
    "\n",
    "def verify_weights(model1, model2, test_inputs):\n",
    "    \"\"\"\n",
    "    Verify that two models produce the same logits given the same input.\n",
    "    Returns True if outputs match within tolerance, False otherwise.\n",
    "    \n",
    "    Args:\n",
    "        model1: First model (HuggingFace implementation)\n",
    "        model2: Second model (LanguageModel implementation)\n",
    "        test_inputs: List of input strings to test\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Set both models to eval mode\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    \n",
    "    matches = []\n",
    "    with torch.no_grad():\n",
    "        for input_text in test_inputs:\n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # Get outputs from both models\n",
    "            output1 = model1(**inputs).logits\n",
    "            output2 = model2.forward(**inputs).logits\n",
    "            \n",
    "            # Compare outputs\n",
    "            max_diff = torch.max(torch.abs(output1 - output2))\n",
    "            matches.append(max_diff < 1e-5)\n",
    "            \n",
    "            print(f\"Max difference in logits for '{input_text}': {max_diff:.2e}\")\n",
    "    \n",
    "    return all(matches)\n",
    "\n",
    "# Example usage\n",
    "test_inputs = [\n",
    "    \"The quick brown fox\",\n",
    "    \"Machine learning is\",\n",
    "    \"Python programming\"\n",
    "]\n",
    "\n",
    "# Copy weights\n",
    "print(\"Copying weights...\")\n",
    "copy_weights(hf_model, model)\n",
    "\n",
    "del hf_model\n",
    "\n",
    "# Verify the transfer\n",
    "print(\"\\nVerifying weight transfer...\")\n",
    "success = verify_weights(hf_model, model, test_inputs)\n",
    "print(f\"\\nWeight transfer {'successful' if success else 'failed'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "# from nnsight import LanguageModel\n",
    "import argparse\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import torch.multiprocessing as mp\n",
    "import time\n",
    "import huggingface_hub\n",
    "from datasets import config\n",
    "import transformer_lens.loading_from_pretrained\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens\n",
    "\n",
    "\n",
    "model_name = \"EleutherAI/pythia-1b\"\n",
    "device = \"cuda\"\n",
    "hf_model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    revision=\"step0\",\n",
    "    cache_dir=\"./pythia-1b/step0\",\n",
    ").to(device, dtype=t.bfloat16)\n",
    "\n",
    "\n",
    "model_name_2 = f\"{model_name}\"\n",
    "\n",
    "model = HookedTransformer.from_pretrained_no_processing(model_name=model_name_2, checkpoint_index=0, dtype=\"bfloat16\")\n",
    "\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = \"Hello, my name is\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output1 = hf_model(**inputs).logits\n",
    "print(output1)\n",
    "\n",
    "output2 = model(inputs[\"input_ids\"])\n",
    "\n",
    "print(output2)\n",
    "print(output1.shape, output2.shape)\n",
    "print(output2.dtype)\n",
    "\n",
    "print(t.allclose(output1, output2, atol=1e-3))\n",
    "\n",
    "diff = t.abs(output1 - output2)\n",
    "print(diff.max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
